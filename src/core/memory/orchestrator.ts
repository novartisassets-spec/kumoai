import { HistoryManager } from './history-manager';
import { embeddingService } from './embedding-service';
import { SummarizerService } from './summarizer-service';
import { logger } from '../../utils/logger';
import { UserRepository } from '../../db/repositories/user.repo';
import { CONSTANTS } from '../../config/constants';

function estimateTokens(text: string): number {
    return Math.ceil(text.length * CONSTANTS.LLM.TOKENS_PER_CHAR_ESTIMATE);
}

function truncateText(text: string, maxTokens: number): string {
    const maxChars = Math.floor(maxTokens / CONSTANTS.LLM.TOKENS_PER_CHAR_ESTIMATE);
    if (text.length <= maxChars) return text;
    return text.substring(0, maxChars);
}

interface TruncationStats {
    originalTokens: number;
    finalTokens: number;
    componentsTruncated: string[];
    preservedComponents: string[];
}

async function countContextTokens(
    semanticContext: string,
    historyContext: string,
    pendingEscalationsContext: string,
    resolvedEscalationsContext: string,
    userInput: string
): Promise<{ [key: string]: number }> {
    return {
        semanticContext: estimateTokens(semanticContext),
        historyContext: estimateTokens(historyContext),
        pendingEscalationsContext: estimateTokens(pendingEscalationsContext),
        resolvedEscalationsContext: estimateTokens(resolvedEscalationsContext),
        userInput: estimateTokens(userInput),
    };
}

export class MemoryOrchestrator {
    /**
     * Prepares context for AI requests - BALANCED FOR DEPTH AND EFFICIENCY
     * 
     * Strategy:
     * 1. LLM-GENERATED SUMMARY: Use ConversationSummarizer output (detailed, semantic)
     * 2. RECENT CONTEXT: Last 9 messages for immediate conversation awareness
     * 3. ACTION AWARENESS: Include completed actions + pending escalations with metadata
     * 4. CONTEXT AWARENESS: Agents see what was done, what's pending, what's next
     * 
     * This keeps agents action-aware and context-aware while staying under token limits
     */
    static async getFullContext(
        schoolId: string, 
        userId: string | undefined, 
        fromPhone: string, 
        userInput: string,
        agent?: 'TA' | 'PA' | 'GA' | 'SA'
    ): Promise<string> {
        try {
            // 0. PRE-FETCH SUMMARIZATION - Check and trigger BEFORE getting context
            // This ensures summaries are available for CURRENT response, not just future ones
            if (userId) {
                await this.checkAndTriggerSummarization(schoolId, userId, fromPhone);
            }

            // 1. DETAILED SUMMARY (generated by LLM via ConversationSummarizer)
            let semanticContext = '';
            if (userId) {
                try {
                    const relevantSummaries = await embeddingService.findRelevantSummaries(userId, userInput, 2); // â† Get up to 2 summaries for context awareness
                    if (relevantSummaries.length > 0) {
                        // These summaries are generated by ConversationSummarizer (LLM-based)
                        semanticContext = `**Context Summary (from conversation memory):**\n${relevantSummaries.join('\n\n---\n\n')}\n`;
                    }
                } catch (summaryErr) {
                    logger.debug({ summaryErr: (summaryErr as any).message }, 'No summaries available yet - starting fresh');
                }
            }

            // 2. RECENT MESSAGES (last 9 for immediate context)
            const recentMessages = await HistoryManager.getSlidingWindow(fromPhone, 9, userId, schoolId); // â† 9 messages = ~15-20 min conversation window, with schoolId for security
            const historyContext = recentMessages.length > 0 
                ? recentMessages.map(m => {
                    const actionInfo = m.action_performed ? ` [Action: ${m.action_performed}]` : '';
                    return `[${m.timestamp}] ${m.sender_role}: ${m.content}${actionInfo}`;
                }).join('\n')
                : '(No recent history - first interaction)';

            // 3. ACTION AND CONTEXT AWARENESS - Handled by V2 'Active Resume'
            // The Dispatcher injects system messages when escalations resume, so agents receive context directly
            // No need for passive polling here - V2 pushes context through active message injection
            let adminInstructions = '';

            // 3.5 ESCALATION CONTEXT - Unified for PA/TA/SA
            // PA/TA see their own pending escalations + recent resolved for pattern awareness
            // SA sees ALL pending escalations across all agents + recent resolved for consistency
            let pendingEscalationsContext = '';
            let resolvedEscalationsContext = '';
            
            try {
                const { EscalationServiceV2 } = await import('../../services/escalation-v2');
                const { EscalationAuditService } = await import('../../services/escalation-audit');
                
                // PENDING ESCALATIONS
                const allPendingEscalations = await EscalationServiceV2.getPendingEscalations(schoolId);
                
                if (agent === 'SA') {
                    // SA sees ALL pending escalations with origin agent info
                    if (allPendingEscalations && allPendingEscalations.length > 0) {
                        pendingEscalationsContext = '\n**ACTIVE ESCALATIONS (Awaiting Your Decision):**\n' +
                            allPendingEscalations.map((esc: any, idx: number) => {
                                const createdTime = new Date(esc.timestamp).toLocaleTimeString('en-NG', { timeZone: 'Africa/Lagos' });
                                return `[${idx + 1}] ID: ${esc.id}\n` +
                                    `    Type: ${esc.escalation_type} | Origin: ${esc.origin_agent}\n` +
                                    `    From: ${esc.from_phone}\n` +
                                    `    Reason: ${esc.reason}\n` +
                                    `    Priority: ${esc.priority}\n` +
                                    `    Created: ${createdTime}\n` +
                                    `    Status: ${esc.escalation_state}`;
                            }).join('\n\n') + 
                            '\n\n[Quick Action] When responding, reference escalation ID explicitly (e.g., "Close ESC-..." or "Approve ID...")';
                    }
                } else if (agent === 'PA' || agent === 'TA') {
                    // PA/TA see only their own pending escalations
                    const agentEscalations = allPendingEscalations?.filter((e: any) => e.origin_agent === agent) || [];
                    if (agentEscalations.length > 0) {
                        // For TA: group escalations by class_level for class-scoped awareness
                        let escalationDisplay = '';
                        
                        if (agent === 'TA') {
                            // Group TA escalations by class level
                            const byClass: { [key: string]: any[] } = {};
                            agentEscalations.forEach((esc: any) => {
                                const classLevel = esc.context?.class_level || 'General';
                                if (!byClass[classLevel]) byClass[classLevel] = [];
                                byClass[classLevel].push(esc);
                            });
                            
                            escalationDisplay = '\n**YOUR PENDING ESCALATIONS BY CLASS (Awaiting Admin Decision):**\n' +
                                Object.entries(byClass).map(([classLevel, escs]) => {
                                    return `ðŸ“š **${classLevel}:**\n` +
                                        escs.map((esc: any, idx: number) => {
                                            const createdTime = new Date(esc.timestamp).toLocaleTimeString('en-NG', { timeZone: 'Africa/Lagos' });
                                            const subtype = esc.context?.escalation_subtype ? ` [${esc.context.escalation_subtype}]` : '';
                                            return `   [${idx + 1}] ID: ${esc.id}${subtype}\n` +
                                                `       Subject: ${esc.context?.subject || 'N/A'}\n` +
                                                `       Reason: ${esc.reason}\n` +
                                                `       Priority: ${esc.priority}\n` +
                                                `       Created: ${createdTime}\n` +
                                                `       Status: ${esc.escalation_state}`;
                                        }).join('\n\n');
                                }).join('\n\n');
                        } else {
                            // PA escalations (not grouped)
                            escalationDisplay = '\n**YOUR PENDING ESCALATIONS (Awaiting Admin Decision):**\n' +
                                agentEscalations.map((esc: any, idx: number) => {
                                    const createdTime = new Date(esc.timestamp).toLocaleTimeString('en-NG', { timeZone: 'Africa/Lagos' });
                                    return `[${idx + 1}] ID: ${esc.id}\n` +
                                        `    Type: ${esc.escalation_type}\n` +
                                        `    Reason: ${esc.reason}\n` +
                                        `    Priority: ${esc.priority}\n` +
                                        `    Created: ${createdTime}\n` +
                                        `    Status: ${esc.escalation_state}`;
                                }).join('\n\n');
                        }
                        
                        pendingEscalationsContext = escalationDisplay;
                    }
                }
                
                // RESOLVED ESCALATIONS (last 7 days - for context/pattern recognition)
                const recentResolved = await EscalationAuditService.getRecentResolvedEscalations(schoolId, 7);
                
                if (agent === 'SA') {
                    // SA sees all resolved escalations (consistency checking)
                    if (recentResolved && recentResolved.length > 0) {
                        resolvedEscalationsContext = '\n**RECENT ESCALATIONS (Resolved - for your reference):**\n' +
                            recentResolved.slice(0, 5).map((esc: any) => {
                                const decidedTime = esc.decided_at ? new Date(esc.decided_at).toLocaleDateString('en-NG') : 'N/A';
                                return `[${esc.origin_agent}] Type: ${esc.escalation_type}\n` +
                                    `    Reason: ${esc.reason}\n` +
                                    `    Decision: ${esc.decision || 'N/A'}\n` +
                                    `    Decided: ${decidedTime}`;
                            }).join('\n\n');
                    }
                } else if (agent === 'PA' || agent === 'TA') {
                    // PA/TA see only their own recent resolved escalations (pattern awareness)
                    const agentResolved = recentResolved?.filter((e: any) => e.origin_agent === agent) || [];
                    if (agentResolved.length > 0) {
                        resolvedEscalationsContext = '\n**YOUR RECENT ESCALATIONS (Resolved - for reference):**\n' +
                            agentResolved.slice(0, 3).map((esc: any) => {
                                const decidedTime = esc.decided_at ? new Date(esc.decided_at).toLocaleDateString('en-NG') : 'N/A';
                                return `[${esc.escalation_type}] ${esc.reason}\n` +
                                    `    Decision: ${esc.decision || 'N/A'}\n` +
                                    `    Decided: ${decidedTime}`;
                            }).join('\n\n');
                    }
                }
            } catch (error) {
                logger.warn({ error, agent, schoolId }, 'Failed to fetch escalation context');
            }

            // 5. ASSEMBLE COMPLETE CONTEXT WITH TOKEN MANAGEMENT
            const tokenBreakdown = await countContextTokens(
                semanticContext,
                historyContext,
                pendingEscalationsContext,
                resolvedEscalationsContext,
                userInput
            );
            
            const totalTokens = Object.values(tokenBreakdown).reduce((sum, t) => sum + t, 0);
            const safetyThreshold = CONSTANTS.LLM.SAFETY_THRESHOLD;
            
            let finalPrompt = '';
            let truncationStats: TruncationStats = {
                originalTokens: totalTokens,
                finalTokens: 0,
                componentsTruncated: [],
                preservedComponents: []
            };
            
            if (totalTokens <= safetyThreshold) {
                finalPrompt = `${semanticContext}${semanticContext ? '---\n\n' : ''}**Recent Conversation (Last 9 messages):**\n${historyContext}${adminInstructions}${pendingEscalationsContext}${resolvedEscalationsContext}\n\n---\n**New User Input:**\n${userInput}`;
                truncationStats.finalTokens = totalTokens;
                truncationStats.preservedComponents = ['semanticContext', 'historyContext', 'escalations', 'userInput'];
            } else {
                logger.warn({ 
                    totalTokens, 
                    safetyThreshold, 
                    tokenBreakdown 
                }, 'âš ï¸ [MemoryOrchestrator] Context exceeds token limit - applying truncation strategy');
                
                // Truncation Strategy (Priority: escalations > userInput > recent messages > summaries)
                // Step 1: Start with user input (highest priority - always needed)
                let remainingTokens = safetyThreshold - tokenBreakdown.userInput;
                let truncatedUserInput = userInput;
                
                // Step 2: Preserve escalations (high priority - context awareness)
                let truncatedPendingEsc = '';
                let truncatedResolvedEsc = '';
                const escTokens = tokenBreakdown.pendingEscalationsContext + tokenBreakdown.resolvedEscalationsContext;
                
                if (escTokens <= remainingTokens * 0.3) {
                    truncatedPendingEsc = pendingEscalationsContext;
                    truncatedResolvedEsc = resolvedEscalationsContext;
                    remainingTokens -= escTokens;
                    truncationStats.preservedComponents.push('escalations');
                } else if (tokenBreakdown.pendingEscalationsContext <= remainingTokens * 0.25) {
                    truncatedPendingEsc = truncateText(pendingEscalationsContext, remainingTokens * 0.25);
                    remainingTokens -= estimateTokens(truncatedPendingEsc);
                    truncationStats.componentsTruncated.push('pendingEscalations');
                }
                
                // Step 3: Recent messages (medium priority)
                let truncatedHistory = '';
                if (tokenBreakdown.historyContext <= remainingTokens * 0.4) {
                    truncatedHistory = historyContext;
                    remainingTokens -= tokenBreakdown.historyContext;
                    truncationStats.preservedComponents.push('historyContext');
                } else {
                    truncatedHistory = truncateText(historyContext, remainingTokens * 0.4);
                    remainingTokens -= estimateTokens(truncatedHistory);
                    truncationStats.componentsTruncated.push('historyContext');
                }
                
                // Step 4: Summaries (lowest priority - can be dropped if needed)
                let truncatedSemantic = '';
                if (remainingTokens > tokenBreakdown.semanticContext) {
                    truncatedSemantic = semanticContext;
                    truncationStats.preservedComponents.push('semanticContext');
                } else if (remainingTokens > 100) {
                    truncatedSemantic = truncateText(semanticContext, remainingTokens);
                    truncationStats.componentsTruncated.push('semanticContext');
                } else {
                    logger.info({ remainingTokens }, 'âš ï¸ [MemoryOrchestrator] Insufficient tokens for summaries - dropping');
                    truncationStats.componentsTruncated.push('semanticContext (dropped)');
                }
                
                finalPrompt = `${truncatedSemantic}${truncatedSemantic ? '---\n\n' : ''}**Recent Conversation:**\n${truncatedHistory}${adminInstructions}${truncatedPendingEsc}${truncatedResolvedEsc}\n\n---\n**New User Input:**\n${truncatedUserInput}`;
                truncationStats.finalTokens = estimateTokens(finalPrompt);
                
                logger.warn({ 
                    truncationStats,
                    originalBreakdown: tokenBreakdown
                }, 'ðŸ“Š [MemoryOrchestrator] Context truncated successfully');
            }

            logger.debug({ 
                contextLength: finalPrompt.length,
                estimatedTokens: truncationStats.finalTokens,
                tokenBreakdown,
                wasTruncated: truncationStats.originalTokens > safetyThreshold,
                truncationStats,
                agent,
                hasSemanticContext: !!semanticContext,
                recentMessageCount: recentMessages.length,
                hasPendingEscalations: !!pendingEscalationsContext,
                hasResolvedEscalations: !!resolvedEscalationsContext
            }, 'ðŸ“Š [MemoryOrchestrator] Context assembled - ACTION + CONTEXT AWARE');

            return finalPrompt;

        } catch (error) {
            logger.error({ error }, 'Failed to assemble context from memory');
            return userInput;
        }
    }

    private static async checkAndTriggerSummarization(schoolId: string, userId: string, fromPhone: string) {
        try {
            const count = await HistoryManager.getMessageCountSinceLastSnapshot(userId);
            if (count >= CONSTANTS.MEMORY.MESSAGES_BEFORE_SUMMARY) {
                logger.info({ userId, count }, 'Synchronously triggering summarization before context fetch');
                
                const fullBlock = await HistoryManager.getSlidingWindow(fromPhone, CONSTANTS.MEMORY.MESSAGES_BEFORE_SUMMARY, userId, schoolId);
                const summary = await SummarizerService.summarizeBlock(fullBlock);
                
                await embeddingService.storeSnapshot(schoolId, userId, summary, count);
                logger.info({ userId }, 'Memory Snapshot stored and available for current response');
            }
        } catch (error) {
            logger.error({ error, userId }, 'Summarization check failed');
        }
    }
}
